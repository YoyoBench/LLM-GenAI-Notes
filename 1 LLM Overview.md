# 1 LLM Overview

Large Language Model

LLMs are built by using supervised learning to repeatedly predict next word.

## 1.1 Abilities

1. ### 涌现能力

   **上下文学习**：这种能力允许语言模型在提供自然语言指令或多个任务示例的情况下，通过理解上下文并生成相应输出的方式来执行任务，而无需额外的训练或参数更新。

   **指令遵循**：即指令微调，LLM 被证明在使用指令形式化描述的未见过的任务上表现良好，所以LLM 能够根据任务指令执行任务，而无需事先见过具体示例，展示了其强大的泛化能力。

   **逐步推理**：LLM 通过采用 思维链（CoT, Chain of Thought） 推理策略，利用包含中间推理步骤的提示机制来解决这些任务，从而得出最终答案。

2. ### 作为基座模型支持多元应用的能力

   多个应用可以只依赖于一个或少数几个大模型进行统一建设。

    `基座模型解释：基座模型（Foundation Model）是指在人工智能和机器学习领域中，预训练在大规模数据集上的通用模型，这些模型具备强大的泛化能力和灵活性，能够在多种任务和领域中发挥作用。基座模型的核心特点是它们在预训练阶段学习到了广泛的知识和语言模式，之后可以通过微调（如指令微调）适应特定任务，从而显著提升各类下游任务的性能。`

3. **支持对话作为统一入口的能力**



## 1.2 Pros and Cons

### Pros

1. **巨大的规模：** LLM 通常具有巨大的参数规模，可以达到数十亿甚至数千亿个参数。这使得它们能够捕捉更多的语言知识和复杂的语法结构。
2. **预训练和微调：** LLM 采用了预训练和微调的学习方法。首先在大规模文本数据上进行预训练（无标签数据），学习通用的语言表示和知识。然后通过微调（有标签数据）适应特定任务，从而在各种 NLP 任务中表现出色。
3. **上下文感知：** LLM 在处理文本时具有强大的上下文感知能力，能够理解和生成依赖于前文的文本内容。这使得它们在对话、文章生成和情境理解方面表现出色。
4. **多语言支持：** LLM 可以用于多种语言，不仅限于英语。它们的多语言能力使得跨文化和跨语言的应用变得更加容易。
5. **多模态支持：** 一些 LLM 已经扩展到支持多模态数据，包括文本、图像和声音。使得它们可以理解和生成不同媒体类型的内容，实现更多样化的应用。

### Cons

1. **Bias & Toxicity：** 尽管 LLM 具有出色的能力，但它们也引发了伦理和风险问题，包括生成有害内容、隐私问题、认知偏差等。因此，研究和应用 LLM 需要谨慎。

2. **Computational Challenges：** LLM 参数规模庞大，需要大量的计算资源进行训练和推理。

   - 通常需要使用高性能的 GPU 或 TPU 集群来实现
   - Reduce required memory to store & train models
   - Projects original 32 bit floating point numbers into lower precision spaces, e.g., **BFLOAT16** is a popular choice
   - Quantization Aware Training (**QAT**) learns the quantization scaling factors during the training
   - Multi-GPU compute: distributed data parallel (**DDP**), fully shared data parallel (**FSDP**)

3. **Hallucinations：** LLM 有时会产生与客观事实不符的信息，导致用户接收到的信息不准确

4. **Knowledge Cutoffs：** LLM 基于静态的数据集训练，无法及时反映最新的信息动态

5. **Limited Context Length：** LLM 在理解和生成长篇内容时受限于有限的上下文窗口，且必须按顺序处理内容，输入越长，速度越慢。

6. **Tabular Data**

7. **内容不可追溯：** LLM 生成的内容往往缺乏明确的信息来源，影响内容的可信度

8. **领域专业知识能力欠缺：** LLM 特定领域的专业知识较少，效果不好

9. **推理能力限制：** 面对复杂问题时，LLM 可能缺乏必要的推理能力，

10. **应用场景适应性受限：** 单一模型可能难以全面适应所有场景

    

## 1.3 Lifecycle

   
![image](https://github.com/YoyoBench/LLM-GenAI-Notes/assets/109169949/e66ee858-5c30-4ab1-968c-9e6ce498aab1)

![image](https://github.com/YoyoBench/LLM-GenAI-Notes/assets/109169949/a4948ec9-5bf0-452e-9f66-641e793e8c00)


1. **确定目标**。在进行开发前，我们首先需要确定开发的目标，即要开发的应用的应用场景、目标人群、核心价值。对于个体开发者或小型开发团队而言，一般应先设定最小化目标，从构建一个 MVP（最小可行性产品）开始，逐步进行完善和优化。

2. **设计功能**。在确定开发目标后，需要设计本应用所要提供的功能，以及每一个功能的大体实现逻辑。虽然我们通过使用大模型来简化了业务逻辑的拆解，但是越清晰、深入的业务逻辑理解往往也能带来更好的 Prompt 效果。同样，对于个体开发者或小型开发团队来说，首先要确定应用的核心功能，然后延展设计核心功能的上下游功能；例如，我们想打造一款个人知识库助手，那么核心功能就是结合个人知识库内容进行问题的回答，那么其上游功能的用户上传知识库、下游功能的用户手动纠正模型回答就是我们也必须要设计实现的子功能。

3. **搭建整体架构**。目前，绝大部分大模型应用都是采用的特定数据库 + Prompt + 通用大模型的架构。我们需要针对我们所设计的功能，搭建项目的整体架构，实现从用户输入到应用输出的全流程贯通。一般来说，我们推荐基于 LangChain 框架进行开发。LangChain 提供了 Chain、Tool 等架构的实现，我们可以基于 LangChain 进行个性化定制，实现从用户输入到数据库再到大模型最后输出的整体架构连接。

4. **搭建数据库**。个性化大模型应用需要有个性化数据库进行支撑。由于大模型应用需要进行向量语义检索，一般使用诸如 Chroma 的向量数据库。在该步骤中，我们需要收集数据并进行预处理，再向量化存储到数据库中。数据预处理一般包括从多种格式向纯文本的转化，例如 PDF、MarkDown、HTML、音视频等，以及对错误数据、异常数据、脏数据进行清洗。完成预处理后，需要进行切片、向量化构建出个性化数据库。

5. **Prompt Engineering**。优质的 Prompt 对大模型能力具有极大影响，我们需要逐步迭代构建优质的 Prompt Engineering 来提升应用性能。在该步中，我们首先应该明确 Prompt 设计的一般原则及技巧，构建出一个来源于实际业务的小型验证集，基于小型验证集设计满足基本要求、具备基本能力的 Prompt。

6. **验证迭代**。验证迭代在大模型开发中是极其重要的一步，一般指通过不断发现 Bad Case 并针对性改进 Prompt Engineering 来提升系统效果、应对边界情况。在完成上一步的初始化 Prompt 设计后，我们应该进行实际业务测试，探讨边界情况，找到 Bad Case，并针对性分析 Prompt 存在的问题，从而不断迭代优化，直到达到一个较为稳定、可以基本实现目标的 Prompt 版本。

7. **前后端搭建**。完成 Prompt Engineering 及其迭代优化之后，我们就完成了应用的核心功能，可以充分发挥大语言模型的强大能力。接下来我们需要搭建前后端，设计产品页面，让我们的应用能够上线成为产品。前后端开发是非常经典且成熟的领域，此处就不再赘述，我们采用 Gradio 和 Streamlit，可以帮助个体开发者迅速搭建可视化页面实现 Demo 上线。

8. **体验优化**。在完成前后端搭建之后，应用就可以上线体验了。接下来就需要进行长期的用户体验跟踪，记录 Bad Case 与用户负反馈，再针对性进行优化即可。




